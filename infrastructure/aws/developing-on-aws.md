# Developing on AWS

## Notes

### Day 1

### Module 1: Introduction to Developing on AWS

- Going to build a notes app:
    - Cognito for auth
    - S3 (DynamoDB) for storing notes and interactions
    - Lambda microservices through API Gateway for CRUD operations on notes
    - Poly for notes to speech
    - etc
- CloudFormation with SAM/CDK is the bedrock of infrastructure as code

### Module 2: Building a Web Application on AWS

- AWS REST API
    - Http(s)
        - Mostly using http 2.0 now
    - SigV4
        - Calculating a signature can be intensive; ideal to avoid handling this yourself
    - IAM Access Key (ID, Secret)
- AWS CloudShell is a browser-based shell
- AWS Cloud9 is an IDE for writing, running, and debugging code
    - Utilizes a browser
    - Comes set up with Code Whisperer and other tooling
        - Code Whisperer is an AI coding companion (GA on 4/13/23)
            - Available on VS Code and IntelliJ and other IDEs
            - Collects your data, but you can opt out of that
            - Generates suggestions/completion
                - Can write a comment and hit Enter and Code Whisperer will suggest the desired code
            - Provides security scanning
- [aws.amazon.com/developer/tools](http://aws.amazon.com/developer/tools) provides tools and docs and various languages

### Module 3: Getting Started with Development on AWS

- SDKS
    - Language binding
    - HTTP request signing
    - Built-in resilience
        - Logic for retries/errors/timeouts
    - Pagination suppport
- SDK APIs (with varying levels of abstraction)
    - Low-level: has one method per service operation (like a 1 to 1 mapping)
    - High-level: has one class per conceptual resource; defines service resources and individual resources (like a 1 to many mapping)
        - High-level is simpler and object-oriented (cleaner to work with), but you get less options coming back in the response
- AWS CLI syntax
    - `(base call) aws (servce) s3 (subcommand) ls (target) s3://mybucket (options) --recursive`
- `help` after any incomplete AWS command will print out docs
    - i.e. aws s3 help will display everything about s3
- Tab x 2 after any incomplete AWS command will print all options to the console
- `aws configure set cli_auto_prompt on` on CloudShell will activate CLI Auto Prompt
    - Pressing Enter after any incomplete AWS command will show GUI to select further options in command
        - Will also pull targets based on your account (great if you don't have resource names on hand)
- Service calls can be synchronous or asynchronous
    - For asynchronous commands, can add `wait` before subcommand to wait for response
        - There are config options for waiting, like timeout
    - Languages in SDK have varying levels of support for waiting; examples:
        - .NET will need to use while loops and a polling mechanism
        - Python allows you to get a waiter and wait on a certain call
- CloudWatch to collect metrics and logs and monitor traffic
- AWS X-Ray provides traces, analysis, and service maps
- AWS Toolkits are available on many common IDEs
    - Makes it easier to create, debug, and deploy

### Module 4: Configuring Permissions for the Development Environment

- IAM: Identity and Access Management
- Permissions are configured on a Role and a User basis
    - Users have permissions and security credentials specific to the User in context (username, password, MFA, access keys, etc)
    - Roles have permissions
- Users authenticate and have base permissions; users assume roles that have more permissions
- Users can be put in User Group buckets (dev, admin, etc)
- AWS recommends using User Groups to manage permissions instead of assigning specific permission to specific users
- Permissions policies can be attached to:
    - Identity (Users, User Groups, and Roles)
    - Resources
    - Services (like EC2 or Lambda)
- Identity-based policies always have EAR:
    - Effect (Allow/Disallow)
    - Action (CRUD operation or subcommand)
    - Resource (AWS Object)
- Resource-based policies always have PEAR:
    - Principal: (Who)
    - Effect: (Allow/Disallow)
    - Action (CRUD operation or subcommand)
    - Resource (AWS Object)
- Policies can also have conditions
    - For example, you can create a resource-based policy for all principals that disallows any operation on S3 buckets unless the IP Address falls within a certain CIDR range
- Permissions Boundaries give you limits and cap what you are allowed to access
    - For example, if your Identity Based Policy says you can do A, B, and C and your Permissions Boundary says you can do C, D, and E, then you can only actually do C
- Permissions consumption:
    - If resource has explicity deny, it is denied
    - If a resource has an explicit allow, it is allowed
    - If there is no explicit deny/allow, it is denied
        - Principle of Least Privilege: permissions deny by default and an explicit deny overrides an explicit allow
- STS: Secure Token Service
- ARN: Amazon Resource Name
- CloudWatch launches with user authenticated with their access key and secret access key
- `aws configure` lets you configure usability settings and also user profiles for log ins
    - For example: `aws configure --profile sourcegraph-admin`
    - Assuming a profile in a command will execute that command in the context of that profile/user
- IAM Policy Simulator enables you to test different actions with different permission policies (user, group, and role)
- `.aws/credentials` holds credentials for profiles
- `.aws/config` holds items like region and output format for profiles
    - Can have general config as well as resource config and API version locks
- AWS_PROFILE is an env variable that lets you set the default profile
- Credential priority order (SigV4):
    - Code or CLI
    - Env variables
    - Default credential profile in `credentials` file
    - Instance profile
- Temporary credentials:
    - short term
    - basis for roles
    - Not stored with requestor
    - Non-recyclable
    - Support web identity federation
- To use instance profile in Cloud9:
    - Run `aws configure` and leave access key and secret access key blank
    - Use AWS Explorer on the left of the IDE to verify you are using the instance profile in the correct region

### Module 5: Storage Options

- Types of storage on AWS Cloud:
    - Block storage (EBS)
        - data is stored in unrelated blocks
        - a change to part of a file only affects that block and not all blocks of the file
        - great for databases
        - lives in 1 availability zone (AZ) attached to 1 EC2 instance
    - File storage (EFS, Amazon FSx for Windows File Server/Lustre)
        - large, shared content storage
        - multiple AZs and can support many EC2 instances
        - FSx for Lustre is for high performance computing
        - FSx for Windows File Server uses SMB3
        - EFS is the simplest servlerless file storage option and scales with you
            - You just pay for the storage you use
            - When you delete content, it will scale down automatically
    - Object storage (S3, S3 Glacier Flexible Retrieval)
        - Saving in objects instead of unrelated blocks
        - Data being saved is converted into objects
        - If you change anything about the file/content, you need to reupload the new object
        - Archives, data warehouses, etc. best for write-once-read-many (WORM) workloads
        - Cheapest
- EBS provides extreme performance and is the most expensive; S3 is less performant and the cheapest option
- S3 Overview:
    - Reliability: 99.99% in SLA
    - Annual Durability: 99.999999999%
        - 3 copies of storage in 3 different AZs (all under the hood)
    - Security
    - Manageability
        - Objects can have up to 10 tags
        - Buckets can have up to 50 tags
    - Performance and Scalability
    - Regional resource
    - Namespace globally unique
        - This is because each object will have a URL associated with it and this must be unique
        - There are additional partitions for the government and for China; bucket names can be repeated across the partitions (i.e. there can be a bucket with the same name in the normal AWS partition as well as the government partition)
    - Flat structure for stored data
        - Folders are really file prefixes
    - Stores objects as unique key-value (where URL is the key)
        - For example: `https://bucket-name.s3.region.amazonaws.com/folder/object.fileExtension`
- S3 Event Notifications
    - New object created
    - Object removed
    - Object restored
    - Object replicated
- For example, you could create a Lambda that unzips zip files and you can create an Event Notification that fires on new objects of type `.zip` and then triggers the unzip lambda function on that new file object
- Each object has a version ID and object locking is supported
    - URL with have version ID as a URL param `...?versionId=uuid`
- A general delete command marks an object and its versions for deletion but does not actually delete them
    - You can permanently delete object versions
        - Can do each one with ARNs and a loop with CLI
        - Can check each and delete via Console GUI
- Can tag objects in S3 that you can reference in permissions policies
- Can set a Bucket (resource-based) Policy which will have PEAR specified
- Access Point Policies break up Bucket Policy
    - You may have a Bucket Policy that gives everyone allow access to run get commands
    - You can then have Access Points that provide specific groups access to specific folders within the S3 bucket
        - You could also specify additional actions specific groups are allowed to take on resources with a given tag
- Bucket Names cannot use spaces because it will ultimately be an endpoint and must be DNS compatible
- As you add Bucket Policies and Access Points, additional URLs will be created with those specific identiers going between `s3` and the `region` name
- `s3` and `s3api` commands can be used interchangeably
    - Only difference in syntax and level of abstraction with the former being faster and more abstracted
    - If `s3` cannot do what you need it to, then try `s3api` as it will have far more fine-grained options
- boto3 clients support you making REST API calls with the SDK
    - best rpactice to `close()` your client after you are finished using it

### Module 6: Storing and Hosting Your Application

- To check if bucket name exists before you create one, use the head bucket request functionality
    - 200 means bucket exists
    - 404 menas bucket does not exist
    - 403 means bucket exists but in another account
- For files bigger than 100 MB, you may want to do a multipart upload; you have to for files bigger than 5 GB
    - Parts are uploaded in parallel
    - Uploads can be paused and resumed
    - This is far easier with the high level commands
    - With low-level commands, there will be 6 different steps/operations
- `head-object` facilitates retrieving metadata about a object
- S3 Select allows you to retrieve only a subset of data from an object by using simple SQL expressions
    - Only works for CSV, JSON, and Apache Parquet
    - Reduces load at application layer as the S3 Select will be processed at the resource level
- Pre-signed URLs allow you to share objects in an S3 bucket for a limited amount of time
    - When time expires, it will result in a 403 access denied
    - `aws s3 pre-sign s3://bucket-name/object.fileExtension <expiryDesignation>`
- S3 buckets can have lifecycle policies (i.e. delete versions older than a certain creation date)
- High-level commands automatically handle multipart uploads and cleanup of incomplete uploads
- Can `cp` or `sync` files between local environment and s3
    - Can `cp` with logic to exclude and include certain files
        - Can use exclue and include together
    - `-recursive` is particularly useful with these operations
    - `sync` will not create duplicates and will delete files in order to sync states between local and s3
- `aws s3 website` can allow you to create a website/endpoint for an S3 bucket
- CORS policies define how web applications should load in resources from other domains
- Some services require pagination for responses

### Day 2

### Module 7: Getting Started with Databases

- AWS database services:
    - Relational (Aurora, RDS - Relational Database Service, Redshift)
    - Non-Relational (more like a JSON doc):
        - Key-value (Dynamo DB)
        - Graph (Neptune)
        - In-memory caching (ElastiCache)
        - There are other options for Wide column, Time series, and Ledger databases among others
- Relational databases scale vertically whereas non-relational databases scale horizontally
    - Vertical Scaling is like a skyscraper where each floor gets more expensive than the last
    - Horizontal Scaling is like an apartment complex where new buildings crop up to house more things and pre-existing things are not affected
- DynamoDB
    - Performance at scale
        - 10 trillion requests per day
    - Enterprise ready
        - 99.999% availability
    - Low-latency queries
        - Prime day had 150 million requests per second and only had single-digit milliseconds of latency
    - Serverless
    - Fully managed
    - Fine-grainted access control
        - IAM policies
        - Encrypts your data
        - Can keep it off the public internet (accessible through VPN)
        - Automatic backups and recovery
- Partition Key in DynamoDB is the ID column
    - Table data is stored based on Partition Key
    - Can have duplicates if a Sort Key is supplied
    - If there is no Sort Key, Partition Key cannot be duplicated
- Each partition has 10 GB max size
- Item Attributes must have:
    - Name
    - Data type
    - Value
- Primary Key is the combination of the Partition Key and Sort Key and must be unique
- Read and Write Throughput are represented as:
    - Read Capacity Unit (RCU):
    - Write Capacity Unit (WCU): Number of 1KB writes per second
- There is a capacity calculator to help you determine DB configuration
- On-Demand Read/Write: you only pay for what you need but it's more expensive
- Provisioned: you allocate capacity ahead of time and it's cheaper
- Eventual consistency (the default) uses half the provisioned read capacity and you may be a second behind realtime
- Secondary Indexes: you can query data based on non-primary key attributes
    - Global secondary indexes:
        - Can have up to 20
        - Can have different Partition and Sort Keys
        - Require a separate RCU and WCU
        - Only support Eventual Consistency as it is essentially another table
    - Local secondary indexes:
        - Can have up to 5
        - Must be created on table creation and cannot be deleted
- Secondary indexes do not need to be unique (think favorited items in a photo album or notes app)
- Hot Partition is a partition which sees far more traffic than others which can be a problem is RCU/WCUs are divided equally
    - Adaptive Capacity and Burst Capacity help deal with this
        - Adaptive Capacity allocates capacity as needed to partitions
        - Burst saves unused partition capacity for 5 minutes to handle spikes in throughput
- You can access DynamoDB in many ways:
    - Console
    - NoSQL Workbench
    - DynamoDB Local (to test out code locally)
    - SDK
    - CLI
    - etc
- When using the CLI to put data into a table, you need to specify data types for attributes
- High-Level Programmatic Interfaces:
    - Object Persistence Interface
        - Data types are mapped
        - Java, .NET
        - Object-centric code
    - Document Interface
        - Data types are implied
        - Java, .NET, Node.js, AWS SDK, etc
- Whether High or Low-Level abstraction, all interactions boil down to AWS API calls
- Unsuccessful requests receive an error code, exception, and a message

### Module 8: Processing Database Transactions

- Choose partition keys on what will distribute queries most evenly
- Can create tables through GUI, SDK, CLI
    - For CLI, you can reference a JSON or YAML document for configuration
- Table creation is an async operation, so you'll want to use waiters when creating through the SDK
- You can later get your table, update certain configurations, and then update the table
- With dynamodb CLI, by default a `put-item` may will overwrite an existing item if the supplied item key already exists in the database
- You can use `batch-write-item` to bulk add (only) new items to a database
- `get-item` is the most performant and matches supplied criteria after a full scan
- There is also a `query` subcommand that can support more complex querying logic
- Scan operations can be divided into multiple threads that scan just a section of the database in parallel
    - If you are often having to do scans, you likely need a secondary index
- `update-item` updates only passed attributes
- Put, Update, and Delete all support conditional write operations
    - "Make this update if this conditional expression is true"
- DynamoDB Caching can be really useful in read-heavy databases
    - ElastiCache is a caching layer (for Redis and other)
    - DynamoDB Accelerator (DAX) is the DynamoDB caching layer that allows you to go from milliseconds of latency to microseconds
- When writing with DAX, the update is written to DAX after being written to the DynamoDB database, so you never need to worry about keeping your cache up to date with writes
    - This means more latency for write-heavy applications
- NoSQL key design conepts:
    - size
    - shape
    - velcocity
- You cannot specify conditions on individual put and delete requests and the `batch-write-item` does not return deleted items

### Module 9: Processing Your Application Logic

- Compute services (abstraction increases with each item in the list):
    - Instances: EC2 - scalable computing capacity
    - Containers: ECS, EKS (fully managed container orchestration)
        - Can run on Fargate which is serverless)
    - Serverless: Lambda (event-driven serverless compute)
- AWS provides base images for ECS and EKS
- You pay per request with Lambda and you pay per resource consumption with everything else
- Lambda:
    - Event Source: changes in resource state, http requests, API calls, etc
    - Function: your code
    - Service: Amazon services and resources on the internet
- Lambas can fire on a huge set of event sources, many of which are events in other AWS services
    - Lambdas are often paired with API Gateway -- for example, some call to the API Gateway will invoke some lambda function
- Anatomy of a Lambda function:
    - Access permissions
    - Triggers: event source
    - Configurations
        - Concurrency
        - Memory
            - If you increase memory, you will use more virtual CPUs (out of your control) and will have a higher cost
            - Can get up to 10GB of ephemeral storage
        - Timeouts
        - etc
    - Code: logic to be invoked
    - Runtime: Java, .NET, Node.js, Python, etc
    - Layers: reuse/share code
        - Can create layers which are basically dependencies you can include in multiple lambdas
            - This can enable you to use your own runtime if you wanted to
- Lambdas can be invoked:
    - Synchronously (with direct invocation and caller waits on lambda response)
    - Asynchronously (pushed to event queue where they are processed as bandwidth allows)
    - Polling (pulled from queue based event source where they are processed as bandwidth allows)
- SnapStart (only for Java right now) offers 10x faster function start up performance (using Firecracker)
- Invocations are routed to a Lambda runtime environment, or a new environment is created
    - Environments will stay active (warm) for about 15 minutes; if Lambda is invoked while there still is a warm environment, that environent is used again; if no warm environment exists, the Lambda will cold start a new environment
- Permissions
    - Invocation Permissions:
        - Grant event source permissions to invoke Lambda (through a resource-based policy)
    - Processing Permissions:
        - Execution Role that grants the Lambda needed resource permissions
- Rekognition is an Amazon service that will tell you what an image is
- Lambda permissions are under Configuration > Permissions
    - By Resource permissions will show resource-based policies that allow event sources to invoke lambda
    - Execution Role can add permissions policies so that the Lambda can access the data it needs to to perform its function
        - For example, if an image upload to an S3 bucket should trigger a Lambda to use Rekognition to return what the image is; resource-based policity will allow S3 object creation to invoke Lambda, and Execution Role policies will allow Lambda to read the object data from S3
- Lambda is stateless:
    - No access to underlying infrastructure
    - External storage is required to persist data from Lambda
- Lambdas can be given function URLs to allow external triggering of the Lambda
    - Can either enforce access control or not
- Handler function is the code to be run upon Lambda invocation
- Event object: data sent during invocation (like function params)
- Context object: methods and properties that provide information about the current runtime environment(like runtime versions)
- Environment variables can be used to set minor configuration details (like bucket names or table names)
    - Lambdas can pull in these environment variables at runtime
    - If one of these values needs to change, the environment variables can be updated without having to redeploy the Lambdas (and this can be done with the CLI with `update-function-configuration`)
- You can use layers from 3rd parties by specifying an ARN
- Serverless Application Model (SAM) allows you to test and debug (step-through) your servlerless applications/functions locally
    - An output file will contain the output from Lambda and the terminal will print out an status code
- If you enable DynamoDB streams on a table, you can associate a Lambda with that stream

### Module 10: Managing the APIs

- API Gateway supports HTTP and REST endpoints as well as WebSocket APIs
    - A private endpoint behind a WAF would need to be REST
    - In general, REST endpoints are more feature-rich
    - WebSocket APIs are bidirectional and use low level protocols
- Dev Features:
    - Host multiple versions
    - Configure API Keys
        - Can be used to limit/monitor access and should not be used for auth
    - Throttle Limits
        - Per zone, user, version, etc
    - Control and manage access
        - Robust access control
    - Data tranformations
    - SDK generation
        - Can export what you build as an SDK
    - Mock integrations
    - Response Caching
- Integrations
    - Proxy (you do not set request or response)
    - Non-proxy (you set request and response)
    - Mock (you configure returned status code)
- API Gateway can provide request mapping, validation, and more
- Can test APIs in Console or through CLI prior to actually deploying them
- Stage (key-value pair) variables can allow you to test with different contexts in Prod vs Dev/Sandbox
- Canary Releases allow you to test out a change with a subset of requests before promoting a change to production fully

### Module 11: An Approach to Building a Modern Application

- Modern applications:
    - Architecture patterns: microservices
    - Software delivery: devops
    - Operational model: serverless
- Monoliths do everything, whereas microservices do one thing adn communicate through API calls
- Monoliths:
    - Does everything
    - Tightly coupled
    - State in each runtime instance
    - Single tech stack
    - Limited data options
    - Organized in layers
    - Deployment complexity
    - Rigid release schedules
    - Operational overhead
- Microservice Architecture
    - Minimal function sercices
    - Deployed separately, but intact together
    - Fit for purpose-based data options
    - Organized around business capabilities
    - State is externalized
    - Choice of tech for each microservice
    - Serverless and automated operational model
- Benefits of Microservices:
    - Development agility
    - Fast and independent deployments
    - Increased security
    - Independent scaling
    - Increased availability and resilience
    - Organized based on business capabilities
- Starting a new dev effort:
    - Enable experimentation by creating a culture of ownership
    - Componentize applications using microservices
        - Well-defined interfaces through APIs
        - Each service is optimized for a single function
        - Each service is independent
    - Domain-driven design
        - microservice is within context of domain
        - defines integration points with other domains
- AWS Compute Optimizer will help you identify if you are under or over taxing your provisioned resources
- AWS Trusted Advisor will give you guidance on optimizing costs (primarily under-utilized resources)
- Big constructs are Regions and there are at least 3 AZs in each Region
    - Servers within a Region are at most 60 miles apart
- Interaction Patterns:
    - API Driven:
        - Synchronous processing
        - Applications and services connect and communicate through APIs
            - AWS services expose their APIs
        - HTTP and HTTPS communication protocol
    - Event-Driven:
        - Asynchronous processing
        - A message represents something that has happened
            - AWs resources can generate events with state changes
        - Consumer subscribes to an event
        - Provider generates an event
- Decoupling a Monolith
    - Start small with simple services to decompose
    - Minimize dependency back to monolith
    - Split complex dependencies early
    - Decouple frequently changing capabilities
    - Common strategy is Strangler Pattern
        - Monolith is put behind API Gateway
        - Functionality is pulled out bit by bit into microservices
        - API Gateway can then direct to monlith or microservice depending on calling context
- Microservices lead to smaller packages/modules that can be built, tested, released, and deployed independently and frequently creating faster feedback loops and reducing overhead for CI/CD pipelines
- Traditional Deployments
    - Provision instance
    - Update OS
    - Install app platform
    - Build and deploy apps
    - Configure auto scaling and load balancing
    - Continuously patch, secure, and monitor servers
    - Monitor and maintain applications
- Serverless Deployments
    - Build and deploy apps
    - Monitor and maintain applications
- Benefits of Serverless Computing
    - No servers to provision or manage
    - Scales with usage
    - Seldom pay for idle servers
    - Availability and fault tolerance is built in
- AWS Health Dashboard can help you troubleshoot any issues with your underlying server architecture
    - There are statuses for services and reports of any degraded service
- AWS Step Functions
    - Orchestration of complex distributed workflows: manage state across distributed tasks while reducing application code and improving resiliency
    - You pay for state transitions (trigger request/event)
    - Coordinate the components of distributed applications using visual workflows that manage:
        - failures
        - retries
        - parallelization
        - service integrations
        - observability
    - Basically seems like the AWS equivalent of a SF Flow
    - Defined in JSON
    - Visual Workflow Builder
- Step functions invoke the task and pause until the task is completed
- Step functions are integrated with most every AWS service

### Module 12: Authenticating Your Application Users

- Amazon Cognito sits in front of your application and handles authentication and authorization
- Authentication is verifying users are who they say they are
- Authorization is verifying whether users can perform the desired action
- Cognito: provides authentication authorization, and user management for your web and mobile apps
- AWS Services in Scope by Compliance Program
- User Pool: sign-up/sign-in process of users and passwords
    - Define user flows
        - Registration
        - Verify email/phone
        - Secure sign-in
        - Forgot password
        - Change password
        - Sign-out
    - Specify security requirements
        - secure password handling
        - MFA
        - enforce password policies
        - encrypt all data server-side
        - support authentication flows
        - scalable to millions of users
- Identity Pool: IAM, roles, access keys
- Wth Cognito you can import users through a CSV and create them manually
- Attributes are metadata that describes individual users
- Groups represent different types of users
- Scope is the level of access to a resource (read/write/etc)
- Cognito issues JWT tokens

### Module 13: Deploying Your Application

- CodeBuild: great for CI pipelines
- CodeDeploy
    - Takes source (CodeCommit for example)
    - Takes repositories
    - Takes deployment type
        - In-Place
        - Blue-Green
    - Targets specific instances based on config
- CodePipeline
    - Takes some settings
    - Takes a stage
    - Takes a build stage
        - Can use CodeBuild or Jenkins
    - Takes a deploy stage
        - Takes action provider
        - Takes application name
        - Takes CodeDeploy group
- CodePipeline can orchestrate CodeCommit, CodeBuild, and CodeDeploy
- Once CodeDeploy groups are set up and CodePipelines are created for deployments to specific groups, code updates can automatically trigger new deployments
- Cloud9 can be set up as a shared environment for multiple devOps engineers to be working on deployments
- AWS CloudFormation enables Infra as Code
- AWS SAM (Serverless Application Model):
    - robust and easy way to do serverless deployments
    - open source framework
    - SAM CLI is built on top of AWS CLI i.e. you need both
    - Transforms and expands deployment into a CloudFormation template
    - Can create:
        - APIs
        - Lambdas (and layers)
        - SimpleTable
        - Step Functions (state machine)
- SAM Setup:
    - Install CLI and get credentials
    - (Optional) Install Docker -- allows local testing
    - Download and install the AWS SAM CLI
- SAM Workflow:
    - Init `sam init`
    - Build `sam build`
    - Test `sam local invoke "FunctionName" -e event.json`
    - Deploy `sam deploy`

### Module 14: Observing Your Application

- Observability is more than uptime monitoring:
    - Visibility into app
    - Real-time troubleshooting
    - Customer experience
    - Performance = Revenue
    - Telemetry data
- 3 Pillars:
    - Logging - record of events
        - CloudWatch Logs
    - Metrics - numerical representation of data used to analyze overall performance and behavior
        - CloudWatch Metrics
    - Tracing - follow user's activity in application to better understand user experience
        - AWS XRay
- Resources that use CloudWatch generate metrics
- CloudWatch is a data points repository
- CloudWatch alarms can send emails or trigger auto-scaling under configured conditions
- CloudWatch Key Components
    - Metric: data about the performance of your system
    - Dimensions: name/value pair that is part of the identity of a metric
    - Alarms: watch a single metric over a specified time period and perform actions based on the metric
    - Logs: AWS provides many types of logs
- Alarms start out with an `insufficient data` state; after a few minutes this should transition to `OK` or `Alarm` based on the metric
- An Alarm can only monitor a single metric; although, a Composite Alarm can monitor multiple metrics
- Application Insights provides a nice dashboard of your various metrics and alarms; great for visibility into your infrastructure and application
- You can create up to 3 custom dashboard for free
- X Ray Console shows a visual representation of the paths and connections between various resources in your infrastructure
    - It will show a stacktrace of sorts with how long each request/interaction took as well as the visual chart
- X Ray has an API reachable via the SDK and CLI

### Module 15: Course Wrap-Up

- Check out AWS Back to Basics videos
- There are more courses
    - Advanced Developing on AWS is the next step up from this class
    - Developing Serverless Solutions on AWS
        - Also intermediate, more demos and labs
    - Getting Started with DevOps on AWS
    - AWS Cloud Development Kit Primer
    - Well-Architected on AWS is a single day course
- AWS Certifications
    - Foundational: Practitioner
    - Associate:
        - Solution Architect
        - Developer
        - SysOps Administrator
    - Professional
        - Solution Architect
        - DevOps Engineer
    - Specialty
        - Advanced Networking
        - Machine Learning
        - Data Analytics
        - Security
        - Database
        - SAP on AWS
- If you get a Professional cert, it will automatically renew lower level certs
- AWS will offer deals on certifications
- Great resources for further learning:
    - `workshops.aws` provides labs for building AWS skills
    - The AWS Certification Quiz Show on YouTube
    - AWS Power Hour is an video series to prep for various certifications
