# Artificial Intelligence & Machine Learning Basics

## Attention Mechanism

Imagine reading a book and highlighting only the important sentences. Attention lets models focus on the most relevant parts of the input, improving performance on tasks like translation or summarization.

## Embedding

Imagine organizing books on shelves so that similar ones are near each other. Embedding maps vectorized data to locations in space so that similar items are close together, making retrieval and comparison efficient.

## Epoch

One epoch is like reading through an entire textbook once while studying. In training, it means the model has seen every example in the dataset one time.

## Evaluation Metrics

Evaluation metrics are the report cards for models. Examples like accuracy, precision, recall, or F1 score tell you how well your model is performing on its tasks.

## Feature

A feature is like a column in a spreadsheet—an individual measurable property or characteristic of your data. For a house, features might include square footage, number of bedrooms, and location.

## Fine-Tuning

Fine-tuning is like taking a pre-trained chef and training them to specialize in vegan recipes. You start with a general model and adapt it to a specific task with new data.

## Gradient Descent

Picture hiking down a mountain in fog. You can't see the entire landscape, so you feel the slope under your feet and take small steps downward. Gradient descent uses this approach to adjust the model little by little to reduce loss.

## Hallucination (in LLMs)

A hallucination is when a model confidently gives an answer that sounds right but is actually false. It's like someone making up facts with a convincing tone but no source.

## Inference

Inference is what happens after training. Like a trained dog responding to commands, a trained model takes new inputs and produces outputs based on what it learned during training.

## Label

The label is the answer you're trying to predict. If features are clues, the label is the mystery you're trying to solve—like predicting the price of a house based on its features.

## Loss Function

Imagine you’re shooting arrows at a bullseye. The loss function is the measure of how far off your arrow lands from the target. Lower loss means you’re getting closer to the bullseye—i.e., your model is making better predictions.

## Model

Think of a model like a recipe. It takes ingredients (input data), follows a specific set of steps (algorithm), and produces a dish (prediction). The better the recipe is written and refined, the better the final outcome.

## Overfitting

Overfitting is like memorizing the answers to a practice test instead of understanding the material. You might ace the practice test but fail the real one. In ML, overfitted models perform well on training data but poorly on new data.

## Prompt Engineering

Prompt engineering is like carefully wording your question to get the best answer from a smart assistant. It’s about crafting inputs that guide a language model to produce useful, accurate results.

## Reinforcement Learning

Think of training a pet: reward good behavior, ignore or correct bad behavior. The model learns by trial and error, getting rewards or penalties based on its actions in an environment.

## Supervised Learning

This is like a teacher guiding a student with answer keys. You train the model using data that includes both the inputs and the correct outputs (labels).

## Tokenization

Tokenization is like breaking a sentence into individual words or phrases—like cutting up a string of beads into smaller segments. Each "token" can be a word, subword, or character used as a unit of processing.

## Transformer

A transformer is like a high-speed assembly line for language tasks. It processes the whole input in parallel and uses attention mechanisms to understand context deeply and efficiently.

## Training

Training is like teaching a child how to solve math problems by showing examples and correcting mistakes. Over time, the child learns the patterns and can solve problems on their own. Similarly, a model improves by learning from labeled examples.

## Underfitting

Underfitting is like not studying enough—your answers are too generic and don’t match either the practice or real tests. An underfit model hasn’t learned enough from the data and makes inaccurate predictions.

## Unsupervised Learning

Here, it’s like giving the student a bunch of puzzles without solutions. The model has to find patterns and structure in the data on its own, like grouping similar items together.

## Vectorization

Think of vectorization as a way to turn each book into a barcode. This barcode is a unique combination of numbers that represents the book’s contents in a machine-readable form.

## Zero-Shot Learning

This is like answering a question about a topic you've never studied based solely on your general knowledge. The model can perform a task it hasn’t seen before by applying broad patterns it has learned.